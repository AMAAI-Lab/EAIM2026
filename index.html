<!DOCTYPE html>
<html lang="en">
   <head>
      <meta charset="utf-8">
      <meta http-equiv="X-UA-Compatible" content="IE=edge">
      <meta name="viewport" content="width=device-width, initial-scale=1">
      <title>Workshop on Deep Learning for Music</title>
      <!-- CSS -->
      <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=Open+Sans:700,300,400">
      <link rel="stylesheet" href="assets/bootstrap/css/bootstrap.min.css">
      <link rel="stylesheet" href="assets/font-awesome/css/font-awesome.min.css">
      <link rel="stylesheet" href="assets/elegant-font/code/style.css">
      <link rel="stylesheet" href="assets/css/animate.css">
      <link rel="stylesheet" href="assets/css/magnific-popup.css">
      <link rel="stylesheet" href="assets/css/form-elements.css">
      <link rel="stylesheet" href="assets/css/style.css">
      <link rel="stylesheet" href="assets/css/media-queries.css">
      <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
      <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
      <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
      <![endif]-->
      <!-- Favicon and touch icons -->
      <link rel="shortcut icon" href="assets/ico/favi.png">
      <!-- <link rel="apple-touch-icon-precomposed" sizes="144x144" href="assets/ico/apple-touch-icon-144-precomposed.png">
         <link rel="apple-touch-icon-precomposed" sizes="114x114" href="assets/ico/apple-touch-icon-114-precomposed.png">
         <link rel="apple-touch-icon-precomposed" sizes="72x72" href="assets/ico/apple-touch-icon-72-precomposed.png">
         <link rel="apple-touch-icon-precomposed" href="assets/ico/apple-touch-icon-57-precomposed.png"> -->
   </head>
   <body>
      <!-- Loader -->
      <div class="loader">
         <div class="loader-img"></div>
      </div>
      <!-- Top menu -->
      <nav class="navbar navbar-fixed-top" role="navigation">
         <div class="container">
            <div class="navbar-header">
               <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#top-navbar-1">
               <span class="sr-only">Toggle navigation</span>
               <span class="icon-bar"></span>
               <span class="icon-bar"></span>
               <span class="icon-bar"></span>
               </button>
               <a class="navbar-brand" href="index.html">Lancar - Bootstrap One-Page Portfolio Template</a>
            </div>
            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="top-navbar-1">
               <ul class="nav navbar-nav navbar-right">
                  <li><a class="scroll-link" href="#top-content">Top</a></li>
                  <li><a class="scroll-link" href="#what-we-do">Description</a></li>
                  <li><a class="scroll-link" href="#portfolio">Speakers</a></li>
                  <li><a class="scroll-link" href="#pricing-2">Call for papers</a></li>
                  <li><a class="scroll-link" href="#about">Organizers</a></li>
                  <li><a class="scroll-link" href="#contact">Contact</a></li>
               </ul>
            </div>
         </div>
      </nav>
      <!-- Page title -->
      <div class="page-title top-content">
         <div class="page-title-text wow fadeInUp">
            <h1> International Workshop on Emerging AI Technologies for Music</h1>
            <p><b>Held in conjunction with the 2025 International Joint Conference on Neural Networks (AAAI 2026)</b></p>
            <p>26th or 27th January 2026<a href="www.ijcnn.org/program-overview" style="color: black;"></a>, Singapore</p>
            <div class="page-title-bottom-link">
               <a class="big-link-1 btn scroll-link" href="#pricing-2"><b>Call for papers</b></a>
               <a class="big-link-2 btn scroll-link" href="#portfolio"><b>Speakers</b></a>
            </div>
         </div>
      </div>
      <!-- What we do -->
      <div class="block-3-container section-container what-we-do-container">
         <div class="container">
            <div class="row">
               <div class="col-sm-12 block-2-left section-description wow fadeIn">
                  <h2>Workshop Description</h2>
                  <div class="divider-1 wow fadeInUp block-2-box "><span></span></div>
                  <p> This full day workshop, part of a new annual series on the latest developments in AI for music, explores the emerging frontier of AI technologies that serve not just as automation tools but as collaborators in creative expression. As AI systems reshape how music is composed, performed, and produced, this workshop highlights the importance of keeping humans actively in the loop through prompt-based interfaces, multimodal workflows and other interactive AI tools. We aim to explore the latest research in human-centric AI for music and discuss how systems can empower humans to guide, steer, and shape AI outputs through intuitive interfaces, interactive workflows, interpretable systems and meaningful user-controls. While early music AI focused on fully autonomous models, the field is increasingly shifting toward co-creative and controllable systems, reflecting growing recognition that meaningful musical outcomes arise through human-machine collaboration. As the inaugural workshop in this planned annual series, we will explore how AI systems can be designed to amplify rather than replace human creativity.
                  </p>
                  <p> We particularly welcome submissions that emphasize controllability, interpretability, explainability, personalization, human-AI interaction and collaboration in music systems. Topics of interest include, but are not limited to:
                  <ul style="font-size: 1.5em; text-align:left; list-style-type:none">
                     <li><span aria-hidden="true" class="icon_plus list-icon"></span>Music composition and generation</li>
                     <li><span aria-hidden="true" class="icon_plus list-icon"></span>Foundational music models</li>
                     <li><span aria-hidden="true" class="icon_plus list-icon"></span>Music production workflows</li>
                     <li><span aria-hidden="true" class="icon_plus list-icon"></span>Music education and pedagogy</li>
                     <li><span aria-hidden="true" class="icon_plus list-icon"></span>Music performance systems</li>
                     <li><span aria-hidden="true" class="icon_plus list-icon"></span>Music information retrieval</li>
                     <li><span aria-hidden="true" class="icon_plus list-icon"></span>Sound design and audio production</li>
                     <li><span aria-hidden="true" class="icon_plus list-icon"></span>Music therapy applications</li>
                     <li><span aria-hidden="true" class="icon_plus list-icon"></span>Music transcription and analysis</li>
                     <li><span aria-hidden="true" class="icon_plus list-icon"></span>Singing voice synthesis</li>
                     <li><span aria-hidden="true" class="icon_plus list-icon"></span>Evaluation of music AI systems</li>
                     <li><span aria-hidden="true" class="icon_plus list-icon"></span>Music recommender systems</li>
                     <li><span aria-hidden="true" class="icon_plus list-icon"></span>Musical instrument design</li>
                     <li><span aria-hidden="true" class="icon_plus list-icon"></span>Robotic musicianship</li>
                     <li><span aria-hidden="true" class="icon_plus list-icon"></span>Optical music recognition</li>
                     <li><span aria-hidden="true" class="icon_plus list-icon"></span>Music theory and musicology</li>
                     <li><span aria-hidden="true" class="icon_plus list-icon"></span>Ethical, cultural and societal implications</li>
                     <li><span aria-hidden="true" class="icon_plus list-icon"></span>Accessibility and inclusion in music AI</li>
                  </ul>
                  </p>
               </div>
            </div>
         </div>
      </div>
      </div>
      <div class="clients-container portfolio-container section-container">
      <div class="container">
         <div class="row">
            <div class="col-sm-12 about section-description wow fadeIn">
               <h2>Speakers: </h2>
               <div class="divider-1 wow fadeInUp"><span></span></div>
            </div>
         </div>
      </div>
      <!-- Block 2  SPEAKERS -->
      <div class="block-2-container section-container about-block-2-container" style="margin-top: -34em;">
         <div class="container">
            <div class="row">
               <div class="col-sm-4 block-2-box block-2-left block-2-media wow fadeInLeft">
                  <div class="block-2-img-container">
                     <img src="assets/img/about/ethan.png" alt="" data-at2x="assets/img/about/ethan.png">
                     <div class="img-container-line line-1"></div>
                     <div class="img-container-line line-2"></div>
                     <div class="img-container-line line-3"></div>
                  </div>
               </div>
               <div class="col-sm-8 block-2-box block-2-right wow fadeInUp">
                  <h3>Dr. Ethan Manilow / <span>Google DeepMind</span> </h3>
                  <!--            <p><b>Using Deep Learning and Reinforcement Learning to Generate Music Audio and Scores</b></p>-->
                  <!--            -->
                  <!--            <p>-->
                  <!--            I'll discuss Project Magenta, an effort to generate music, video, images and text using machine intelligence. Magenta poses the question, ``Can machines make music and art? If so, how? If not, why not?" The goal of Magenta is to produce open-source tools and models that help creative people be even more creative. An overview of Magenta will be given with focus on closing the loop between musicians and code. I'll discuss recent progress in audio and music score generation, and will focus on the challenge of improving machine learning generative models based on user and artist feedback.-->
                  <!--            -->
                  <!---->
                  <!---->
                  <!--                </p>-->
                  <!--                -->
                  <p>
                     <b>Bio</b>
                     Ethan is currently a Senior Research
                     Scientist at Google DeepMind on the Magenta Team. He finished his PhD in Computer Science working under Bryan Pardo in the Interactive Audio Lab at Northwestern University. During his PhD, he spent two years as a Student Researcher with Magenta and prior to that, he spent a year and half as Student Researcher at MERL on the Speech and Audio Team. His research centers on making machine learning systems that listen to and understand musical audio in an effort to make tools that can better assist artists.                
                  </p>
               </div>
            </div>
            <!--<p>test test<p>-->
         </div>
      </div>
      <!-- Block 2 -->
      <div class="block-2-container section-container section-container-gray about-block-2-container">
      <div class="container">
         <div class="row">
            <div class="col-sm-8 block-2-box block-2-left wow fadeInLeft">
               <h3>Dr. Elio Quinton / <span>Universal Music Group</span></h3>
               <!--            <p><p><b>Deep Learning at Pandora</b></p>-->
               <!--            <p>-->
               <!--            -->
               <!--            Music streaming services nowadays offer large catalogues that expose new challenges in terms of automatically recommending music to tens of millions of users. At Pandora, around 75 billion "thumbs" and manual analyses for over 1.5 million songs have been gathered over the years. In this talk we will leverage some of these data by applying deep networks to both audio content and metadata with the goal of improving the listener experience. The basics of collaborative filtering and machine listening will be reviewed, framed under the music recommendation umbrella, and enhanced with various deep learning techniques.-->
               <!--            -->
               <!--                </p>-->
               <!--                -->
               <p>
                  <b>Bio</b>
                  Elio is a scientist, engineer and leader with over a decade of experience in Artificial Intelligence, Machine Learning, and Audio Technology. Currently VP of Artificial Intelligence at Universal Music Group (UMG) and advisor to creative AI startups, he founded and leads the Music & Audio Machine Learning Lab (MAML), the first ever Machine Learning R\&D group in the recorded music industry. MAML's mission is to invent and build next-generation AI/ML tools to support and empower artists and industry professionals globally. Trained as both a scientist, engineer and musician, Elio holds a PhD in ML and Audio DSP from the Center for Digital Music, a Physics MSc, a Music Technology MA, and a diploma in Commercial Music performance from BIMM London.
               </p>
            </div>
            <div class="col-sm-4 block-2-box block-2-left wow fadeInLeft">
               <div class="block-2-img-container">
                  <img src="assets/img/about/elio2.png" alt="" data-at2x="assets/img/about/elio2.png">
                  <div class="img-container-line line-1"></div>
                  <div class="img-container-line line-2"></div>
                  <div class="img-container-line line-3"></div>
               </div>
            </div>
         </div>
      </div>
      <!-- Block 2 (team member) -->
      <div class="block-2-container section-container about-block-2-container">
         <div class="container">
            <div class="row">
               <div class="col-sm-4 block-2-box block-2-left block-2-media wow fadeInLeft">
                  <div class="block-2-img-container">
                     <img src="assets/img/about/dorien.png" alt="" data-at2x="assets/img/about/dorien.png">
                     <div class="img-container-line line-1"></div>
                     <div class="img-container-line line-2"></div>
                     <div class="img-container-line line-3"></div>
                  </div>
               </div>
               <div class="col-sm-8 block-2-box block-2-right wow fadeInUp">
                  <h3>Prof. Dorien Herremans/ <span>Singapore University of Technology and Design</span> </h3>
                  <!--            <p><b>Researching at the intersection of neural networks and music cognition</b></p>-->
                  <!--            <p>-->
                  <!--            One brach of my current research seeks to simulate perceptual and cognitive processes in the domain of music through the use of both shallow and deep neural networks. My talk will discuss how NNs may be employed to model how humans learn tonal structure in music, as well as octave equivalence and harmonic expectation. Focus will be placed on how different representations, training corpora, and architectures may be used to simulate different populations of listeners (e.g., novice versus expert musicians).-->
                  <!--            By taking this interdisciplinary approach between music, cognitive science, and computer science, we may both develop better computational tools and shed light on various aspects of music perception and cognition.-->
                  <!---->
                  <!--                </p>-->
                  <p>
                     <b>Bio</b>
                     Dorien is an Associate Professor at Singapore University of Technology and Design (SUTD), where she leads the Audio, Music, and AI (AMAAI) Lab. Before joining SUTD, she was a Marie Sklodowska-Curie Postdoctoral Fellow at the Centre for Digital Music at Queen Mary University of London. She was also nominated on the Singapore 100 Women in Tech list in 2021, and one of the top 30 SAIL Award (Super AI Leader) Finalists in 2024 at the World AI Conference.
                  </p>
                  <!--<h3>Follow Her:</h3>
                     <div class="about-social">
                         <a href="#"><i class="fa fa-facebook"></i></a>
                         <a href="#"><i class="fa fa-dribbble"></i></a>
                         <a href="#"><i class="fa fa-twitter"></i></a>
                         <a href="#"><i class="fa fa-pinterest"></i></a>
                     </div>-->
               </div>
            </div>
            <!--        <p>-->
            <!--            Claritas est etiam processus dynamicus, qui sequitur mutationem consuetudium lectorum.-->
            <!--            Mirum est notare quam littera gothica, quam nunc putamus parum claram lorem.-->
            <!--</p>-->
         </div>
      </div>
      <!-- Block 2 -->
      <div class="block-2-container section-container section-container-gray about-block-2-container">
         <div class="container">
            <div class="row">
               <div class="col-sm-8 block-2-box block-2-left wow fadeInLeft">
                  <h3>Dr. Hanoi Hantrakul / <span>ex-Google/ByteDance/Doubao</span></h3>
                  <!--            <p><p><b>Deep Learning at Pandora</b></p>-->
                  <!--            <p>-->
                  <!--            -->
                  <!--            Music streaming services nowadays offer large catalogues that expose new challenges in terms of automatically recommending music to tens of millions of users. At Pandora, around 75 billion "thumbs" and manual analyses for over 1.5 million songs have been gathered over the years. In this talk we will leverage some of these data by applying deep networks to both audio content and metadata with the goal of improving the listener experience. The basics of collaborative filtering and machine listening will be reviewed, framed under the music recommendation umbrella, and enhanced with various deep learning techniques.-->
                  <!--            -->
                  <!--                </p>-->
                  <!--                -->
                  <p>
                     <b>Bio</b>
                     Lamtharn "Hanoi" Hantrakul is an AI Research Scientist and AI Sound Artist based in Bangkok, Thailand. His work explores how machine learning can empower music, arts and culture, particularly from Southeast Asia. With over 8 years of experience in the tech industry, he has developed state-of-the-art Generative AI models at Google, TikTok and ByteDance as a Senior AI Research Scientist. He is co-inventor of notable technologies including Google's open source DDSP library and the music large language model SEED-MUSIC deployed in Doubao (China's ChatGPT).
                     As a sound artist performing under "yaboihanoi," his electronic music incorporates Thai tunings and rhythms. He won the 2022 international AI Song Contest with "Enter Demons and Gods" and has performed at SONAR Music Festival alongside artists like Skrillex and Four Tet. His musical instrument "Fidular" received an A' Silver Award and Core77 Design Award in 2017 and is permanently exhibited at the Musical Instruments Museum in Phoenix, AZ. His work on machine learning and cultural empowerment has been covered by international media including Deutschlandfunk, Scientific American and Fast Company.
                  </p>
               </div>
               <div class="col-sm-4 block-2-box block-2-left wow fadeInLeft">
                  <div class="block-2-img-container">
                     <img src="assets/img/about/hanoi.jpeg" alt="" data-at2x="assets/img/about/hanoi.jpeg">
                     <div class="img-container-line line-1"></div>
                     <div class="img-container-line line-2"></div>
                     <div class="img-container-line line-3"></div>
                  </div>
               </div>
            </div>
         </div>
      </div>

      <div class="our-motto-container section-container section-container-full-bg">
         <div class="container">
            <div class="row">
               <div class="col-sm-12 our-motto section-description wow fadeInLeftBig">
                  <p>
                     <a href="https://groups.google.com/forum/#!forum/icdlm" style="color: gray; font-weight: bold;">Join the mailing list for the online community of Deep Learning and Music: </a>
                  </p>
                  <div class="our-motto-author"><a href="https://groups.google.com/forum/#!forum/icdlm" style="font-weight: bold;">ICDLM</a></div>
               </div>
            </div>
         </div>
      </div>
      <!-- Pricing -->
      <div class="pricing-2-container section-container">
         <div class="container">
            <div class="row">
               <div class="col-sm-12 pricing-2 section-description wow fadeIn">
                  <h2>Call for papers</h2>
                  <div class="divider-1 wow fadeInUp"><span></span></div>
                  <p>
                     <b>Submission Requirements</b><br/>
                     Maximum 6 pages in LNCS format. Work-in-progress submissions are welcome; authors are encouraged to include prototype descriptions (conceptual designs without practical implementation evidence are discouraged).
                     We particularly welcome submissions that emphasize controllability, interpretability, explainability, personalization, human–AI interaction and collaboration in music systems.                     
                     <!-- Papers of up to 5 pages using the following template are welcomed for a talk. Submissions will be evaluated according to their originality, technical soundness and relevance to the workshop. The guidelines in of the workshop’s latex <a href="./dlm17template.zip">template</a> should be followed. Contributions should be in PDF format and submitted to d.herremans@qmul.ac.uk with the subject line: [DLM17 paper submission]. Submissions do not need to be anonymized. Papers will be peer-reviewed and published in the proceedings of the workshop.
                        </p><p>
                        <b>Submissions of Abstracts</b><br/>
                        
                        Structured abstracts of max 2 pages can be submitted for a shorter talk. The abstracts should follow the same <a href="./dlm17template.zip">template</a> as the papers and will be included in the proceedings. Abstracts should be in PDF format and submitted to d.herremans@qmul.ac.uk with the subject line: [DLM17 abstract submission]. Abstracts will be peer-reviewed and included in the proceedings of the workshop.
                        </p><p>
                        <b>Special Issue in Journal</b><br/>
                        Authors will be invited to submit a full paper version of their extended abstract for a special issue on deep learning for music and audio in Springer's Neural Computing Applications.  -->
                  </p>
            
                <section class="section" aria-labelledby="publishing">
                  <h2 id="publishing" style="font-size:1rem; margin-bottom:8px;">Publishing venue</h2>
                  <p class="muted">Proceedings will be arranged with the publisher (to be announced later).</p>
                </section>
                  <p>
                     <b>Programme Committee</b><br/>
                     Prof. Dorien Herremans, Singapore University of Technology and Design (editor/reviewer) <br/>
                     Keshav Bhandari, PhD candidate, Queen Mary University of London (editor/reviewer)<br/>
                     Dr. Abhinaba Roy, Singapore University of Technology and Design (editor/reviewer)<br/>
                     Prof. Simon Colton, Queen Mary University of London (editor/reviewer)<br/>
                     Prof. Mathieu Barthet, Queen Mary University of London (editor/reviewer)<br/>
                     Dr. Benjamin Hayes, Sony CSL, Paris (reviewer)<br/>
                     Dr. Jaeyong Kang, Singapore University of Technology and Design (reviewer)<br/>
                     Dr. Berker Banar, Ellison Institute of Technology Oxford (reviewer)<br/>
                     <br/>
                  </p>
                  <p>
                     <b>Important Dates</b><br/>
                     Paper Submission Deadline: tbd<br/>
                     Acceptance Notification: tbd<br/>
                     Final versions due: tbd<br/>
                     Workshop Date: 26 or 27 Jan 2026<br/>
                  </p>
                  <p>
                     <b>Registration</b><br/>
                     Workshop registration will be handled by the main conference, please check <a href="https://aaai.org/conference/aaai/aaai-26/">AAAI</a> for more details.
                  </p>
               </div>
            </div>
         </div>
      </div>
      <!-- About us -->
      <div class="about-container section-container">
         <div class="container">
            <div class="row">
               <div class="col-sm-12 about section-description wow fadeIn">
                  <h2>Organizers</h2>
                  <div class="divider-1 wow fadeInUp"><span></span></div>
                  <p>
                  </p>
               </div>
            </div>
         </div>
      </div>
      <!-- Block 2 (team member) -->
      <div class="block-2-container section-container about-block-2-container">
         <div class="container">
            <div class="row">
               <div class="col-sm-4 block-2-box block-2-left block-2-media wow fadeInLeft">
                  <div class="block-2-img-container">
                     <img src="assets/img/about/keshav.jpg" alt="" data-at2x="assets/img/about/keshav.jpg">
                     <div class="img-container-line line-1"></div>
                     <div class="img-container-line line-2"></div>
                     <div class="img-container-line line-3"></div>
                  </div>
               </div>
               <div class="col-sm-8 block-2-box block-2-right wow fadeInUp">
                  <h3>Keshav Banhari / <span>Queen Mary University of London</span></h3>
                  <p>
                     Keshav is a PhD candidate at the Centre for Digital Music (C4DM) at Queen Mary University of London supervised by Prof. Simon Colton. His research explores neuro-symbolic methods for music composition with a focus on musical structure and controllability. Prior to Queen Mary, he was part of the Interactive Audio Lab at Northwestern University, Evanston. Over the years, Keshav has published papers across conferences such as NeurIPS, AAAI, IJCNN and recently won the best paper award at EvoMUSART-25 (part of EvoStar). Keshav will be the main contact for the proposed AI music workshop at AAAI-26.
                  </p>
               </div>
            </div>
         </div>
      </div>
      <!-- Block 2 (team member) -->
      <div class="block-2-container section-container section-container-gray about-block-2-container">
         <div class="container">
            <div class="row">
               <div class="col-sm-8 block-2-box block-2-left wow fadeInLeft">
                  <h3>Dr. Abhinaba Roy / <span>Singapore University of Technology and Design</span></h3>
                  <p>
                     Abhinaba is a senior research fellow at Singapore University of Technology and Design. He received his Ph.D. in Computer Vision in 2019 from the Istituto Italiano di Tecnologia, Genoa, Italy. He has held positions in both industry and academia, focusing on developing and deploying practical AI solutions. In recent years, his research has increasingly focused on the intersection of AI and music, particularly in areas such as text-to-music generation, symbolic music creation, and multimodal music understanding. His work has been published in leading conferences including ISMIR, IJCNN, AAAI, and others.
                  </p>
               </div>
               <div class="col-sm-4 block-2-box block-2-right block-2-media wow fadeInUp">
                  <div class="block-2-img-container">
                     <img src="assets/img/about/roy.jpeg" alt="" data-at2x="assets/img/about/roy.jpeg">
                     <div class="img-container-line line-1"></div>
                     <div class="img-container-line line-2"></div>
                     <div class="img-container-line line-3"></div>
                  </div>
               </div>
            </div>
         </div>
         <!-- Block 2 (team member) -->
         <div class="block-2-container section-container about-block-2-container">
            <div class="container">
               <div class="row">
                  <div class="col-sm-4 block-2-box block-2-left block-2-media wow fadeInLeft">
                     <div class="block-2-img-container">
                        <img src="assets/img/about/simon.jpeg" alt="" data-at2x="assets/img/about/simon.jpeg">
                        <div class="img-container-line line-1"></div>
                        <div class="img-container-line line-2"></div>
                        <div class="img-container-line line-3"></div>
                     </div>
                  </div>
                  <div class="col-sm-8 block-2-box block-2-right wow fadeInUp">
                     <h3>Prof. Simon Colton / <span>Queen Mary University of London</span></h3>
                     <p>
                        Simon is a Professor of Computational Creativity, AI and Games in EECS at Queen Mary University of London. He was previously an EPSRC leadership fellow at Imperial College and Goldsmiths College, and held an ERA Chair at Falmouth University. He is an AI researcher with around 200 publications whose work has won national and international awards, and has led numerous EPSRC and EU-funded projects. He focuses specifically on questions of Computational Creativity, where researchers study how to engineer systems to take on creative responsibilities in generative music, arts and science projects. Prof. Colton has has written about the philosophy of Computational Creativity and led numerous public engagement projects.
                     </p>
                  </div>
               </div>
            </div>
         </div>
         <!-- Block 2 (team member) -->
         <div class="block-2-container section-container section-container-gray about-block-2-container">
            <div class="container">
               <div class="row">
                  <div class="col-sm-8 block-2-box block-2-left wow fadeInLeft">
                     <h3>Prof. Dorien Herremans / <span>Singapore University of Technology and Design</span></h3>
                     <p>
                        Dorien is an Associate Professor at Singapore University of Technology and Design (SUTD), where she leads the Audio, Music, and AI (AMAAI) Lab. Before joining SUTD, she was a Marie Sklodowska-Curie Postdoctoral Fellow at the Centre for Digital Music at Queen Mary University of London. She was also nominated on the Singapore 100 Women in Tech list in 2021, and one of the top 30 SAIL Award (Super AI Leader) Finalists in 2024 at the World AI Conference.
                     </p>
                  </div>
                  <div class="col-sm-4 block-2-box block-2-right block-2-media wow fadeInUp">
                     <div class="block-2-img-container">
                        <img src="assets/img/about/dorien.png" alt="" data-at2x="assets/img/about/dorien.png">
                        <div class="img-container-line line-1"></div>
                        <div class="img-container-line line-2"></div>
                        <div class="img-container-line line-3"></div>
                     </div>
                  </div>
               </div>
            </div>
         </div>
         <!-- Contact us (block 2) -->
         <div class="block-2-container section-container contact-container">
            <div class="container">
               <div class="row">
                  <div class="col-sm-12 block-2 section-description wow fadeIn">
                     <h2>Contact us</h2>
                     <div class="divider-1 wow fadeInUp"><span></span></div>
                     <p>
                        You can contact the organizers through k [dot] bhandari [a] qmul.ac.uk.
                     </p>
                  </div>
               </div>
            </div>
         </div>
      </div>
      <!-- Footer -->
      <footer>
         <div class="container">
            <div class="row">
               <div class="col-sm-12">
                  <div class="scroll-to-top">
                     <a href="#"><i class="fa fa-chevron-up"></i></a>
                  </div>
               </div>
            </div>
            <div class="row">
               <div class="col-sm-7 footer-copyright">
                  &copy; Workshop for Emerging AI Technologies for Music (EAIM2026)</a>.
               </div>
               <div class="col-sm-5 footer-social">
                  <!-- <a href="#"><i class="fa fa-facebook"></i></a>
                     <a href="#"><i class="fa fa-dribbble"></i></a>
                     <a href="#"><i class="fa fa-twitter"></i></a>
                     <a href="#"><i class="fa fa-google-plus"></i></a>
                     <a href="#"><i class="fa fa-pinterest"></i></a> -->
               </div>
            </div>
         </div>
      </footer>
      <!-- Javascript -->
      <script src="assets/js/jquery-1.11.1.min.js"></script>
      <script src="assets/bootstrap/js/bootstrap.min.js"></script>
      <script src="assets/js/jquery.backstretch.min.js"></script>
      <script src="assets/js/wow.min.js"></script>
      <script src="assets/js/retina-1.1.0.min.js"></script>
      <script src="assets/js/jquery.magnific-popup.min.js"></script>
      <script src="assets/js/waypoints.min.js"></script>
      <script src="assets/js/jquery.countTo.js"></script>
      <script src="assets/js/masonry.pkgd.min.js"></script>
      <script src="assets/js/scripts.js"></script>
      <!--[if lt IE 10]>
      <script src="assets/js/placeholder.js"></script>
      <![endif]-->
   </body>
</html>
