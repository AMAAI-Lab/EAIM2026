<!DOCTYPE html>
<html lang="en">
   <head>
      <meta charset="utf-8">
      <meta http-equiv="X-UA-Compatible" content="IE=edge">
      <meta name="viewport" content="width=device-width, initial-scale=1">
      <title>1st Workshop on Emerging AI Technologies for Music</title>
      <!-- CSS -->
      <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:700,300,400">
      <link rel="stylesheet" href="assets/bootstrap/css/bootstrap.min.css">
      <link rel="stylesheet" href="assets/font-awesome/css/font-awesome.min.css">
      <link rel="stylesheet" href="assets/elegant-font/code/style.css">
      <link rel="stylesheet" href="assets/css/animate.css">
      <link rel="stylesheet" href="assets/css/magnific-popup.css">
      <link rel="stylesheet" href="assets/css/form-elements.css">
      <link rel="stylesheet" href="assets/css/style.css">
      <link rel="stylesheet" href="assets/css/media-queries.css">
      <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
      <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
      <![endif]-->
      <!-- Favicon -->
      <link rel="shortcut icon" href="assets/ico/favi.png">
	   <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-W2W1BTQQ0R"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-W2W1BTQQ0R');
</script>
   </head>
   <body>
      <!-- Loader -->
      <div class="loader">
         <div class="loader-img"></div>
      </div>
      <!-- Top menu -->
      <nav class="navbar navbar-fixed-top" role="navigation">
         <div class="container">
            <div class="navbar-header">
               <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#top-navbar-1">
               <span class="sr-only">Toggle navigation</span>
               <span class="icon-bar"></span>
               <span class="icon-bar"></span>
               <span class="icon-bar"></span>
               </button>
               <a class="navbar-brand" href="index.html">EAIM Workshop</a>
            </div>
            <div class="collapse navbar-collapse" id="top-navbar-1">
               <ul class="nav navbar-nav navbar-right">
                  <li><a class="scroll-link" href="#top-content">Top</a></li>
                  <li><a class="scroll-link" href="#what-we-do">Description</a></li>
                  <li><a class="scroll-link" href="#cfp">Call for Papers</a></li>
                  <li><a class="scroll-link" href="#portfolio">Speakers</a></li>
                  <li><a class="scroll-link" href="#about">Organizers</a></li>
                  <li><a class="scroll-link" href="#contact">Contact</a></li>
               </ul>
            </div>
         </div>
      </nav>
      <!-- Page title -->
      <div class="page-title top-content">
         <div class="page-title-text wow fadeInUp">
            <h1>1st International Workshop on Emerging AI Technologies for Music</h1>
            <p><b>Held in conjunction with AAAI-26</b></p>
            <p>26–27 January 2026, Singapore</p>
            <div class="page-title-bottom-link">
               <a class="big-link-1 btn scroll-link" href="#cfp"><b>Call for Papers</b></a>
               <a class="big-link-2 btn scroll-link" href="#portfolio"><b>Speakers</b></a>
            </div>
         </div>
      </div>
      <!-- Workshop Description -->
      <div class="block-3-container section-container what-we-do-container" id="what-we-do">
         <div class="container">
            <div class="row">
               <div class="col-sm-12 block-2-left section-description wow fadeIn">
                  <!-- New centered H1 with logo -->
				  <h1 style="text-align:center; margin-bottom:40px;">
				   <img src="assets/img/eaim.jpeg" alt="EAIM Workshop Logo"
					style="max-width:660px; width:100%; height:auto; display:block; margin:0 auto 20px;">
				   1st International Workshop on Emerging AI Technologies for Music
				  </h1>
				  <br></br>
				  <h3>Workshop Description</h3>
                  <div class="divider-1 wow fadeInUp"><span></span></div>
                  <p>
                     This full day workshop, part of a new annual series on the latest developments in AI for music, explores how AI can become a true creative collaborator rather than just another automation tool. As AI transforms music composition, performance, and production, we're seeing an exciting shift from fully autonomous systems toward human-centered approaches. We aim to explore the latest research in human-centric AI for music and discuss how systems can empower humans to shape AI outputs through intuitive interfaces, interactive workflows, interpretable systems and meaningful user-controls. As the field is increasingly shifting toward co-creative and controllable systems, we’ll dive into cutting-edge research and discuss how meaningful musical outcomes can arise through human-machine collaboration. Join us for this inaugural workshop as we explore how to design AI that enhances rather than overshadows human musical creativity.
                  </p>
               </div>
            </div>
         </div>
      </div>


      <!-- Call for Papers -->
       <div class="cfp-container section-container">
         <div class="container">
            <div class="row">
               <div class="col-sm-12 cfp section-description wow fadeIn">
                  <h3>Call for Papers</h3>
                  <div class="divider-1 wow fadeInUp"><span></span></div>
                  <p>
                     <b>Submission Requirements</b><br/>
                     We welcome submissions of original research, creative systems, and critical perspectives on emerging AI technologies for music. 
                     Contributions may include theoretical work, empirical studies, system designs, evaluations, or artistic explorations. 
                     While implemented systems and demonstrations are encouraged, strong conceptual and analytical contributions without a prototype are also welcome. 
                     We particularly value submissions that emphasize controllability, interpretability, explainability, personalization, human-AI interaction, and collaboration in music systems.

                     <!-- Conceptual designs without evidence of implementation are discouraged. We particularly welcome submissions that emphasize controllability, interpretability, explainability, personalization, human–AI interaction, and collaboration in music systems. -->
                  </p>
                  <p><b>Topics of Interest include, but are not limited to:</b></p>
                  <ul style="font-size:1.5em; text-align:left; list-style-type:none">
                     <li><span class="icon_plus list-icon"></span>Music composition and generation</li>
                     <li><span class="icon_plus list-icon"></span>Foundational music models</li>
                     <li><span class="icon_plus list-icon"></span>Music production workflows</li>
                     <li><span class="icon_plus list-icon"></span>Music education and pedagogy</li>
                     <li><span class="icon_plus list-icon"></span>Music performance systems</li>
                     <li><span class="icon_plus list-icon"></span>Music information retrieval</li>
                     <li><span class="icon_plus list-icon"></span>Sound design and audio production</li>
                     <li><span class="icon_plus list-icon"></span>Music therapy applications</li>
                     <li><span class="icon_plus list-icon"></span>Music transcription and analysis</li>
                     <li><span class="icon_plus list-icon"></span>Singing voice synthesis</li>
                     <li><span class="icon_plus list-icon"></span>Evaluation of music AI systems</li>
                     <li><span class="icon_plus list-icon"></span>Music recommender systems</li>
                     <li><span class="icon_plus list-icon"></span>Musical instrument design</li>
                     <li><span class="icon_plus list-icon"></span>Robotic musicianship</li>
                     <li><span class="icon_plus list-icon"></span>Optical music recognition</li>
                     <li><span class="icon_plus list-icon"></span>Music theory and musicology</li>
                     <li><span class="icon_plus list-icon"></span>Ethical, cultural and societal implications</li>
                     <li><span class="icon_plus list-icon"></span>Accessibility and inclusion in music AI</li>
                  </ul>
					<script src="https://cdn.logwork.com/widget/countdown.js"></script>
					<a href="https://logwork.com/countdown-timer" class="countdown-timer" data-style="circles" data-timezone="Asia/Singapore" data-date="2025-10-29 23:00">Countdown Timer</a>
				   <script>
					  const iframe = document.getElementById("countdown_iframe_492508");
					
					  iframe.addEventListener("load", () => {
					    try {
					      // Access the iframe's document
					      const doc = iframe.contentDocument || iframe.contentWindow.document;
					      if (doc && doc.body && doc.body.innerText.includes("Incorrect embedding code")) {
					        iframe.style.display = "none";
					        console.warn("Countdown iframe hidden — incorrect embed detected.");
					      }
					    } catch (e) {
					      // Cross-origin protection: if we can’t access the content, do nothing
					      console.log("Unable to inspect iframe content (cross-origin).");
					    }
					  });
					</script>
                  <p>
                     <b>Important Dates</b><br/>
					  Paper Submission Deadline: <s>24 October 2025</s> <b>Extended to 29 October 2025 (Wednesday)</b><br/>
					  Acceptance/Rejection Notice: 14 November 2025<br/>
					  Camera-Ready Deadline: 14 December 2025<br/>
					  Workshop Date: 26–27 January 2026
                  </p>
                  <p>
                     <b>Publishing Venue</b><br/>
                     Proceedings will be published as a volume in Proceedings of Machine Learning Research (PMLR).
                  </p>
				  <p>
				   <b>Submission Instructions</b><br/>
				   Paper format: Maximum of 11 pages (excluding references). Please follow the PMLR format using this template: 
				    <a href="https://www.overleaf.com/latex/templates/eaim-template/bvqzcsngcxwz" target="_blank" style="font-weight:bold;  background-color:#2BA560; color:white;">
					   EAIM Template on Overleaf
					</a>
				   <br/><br/>
				   Submission Portal: 
				   <a href="https://openreview.net/group?id=AAAI.org/2026/Workshop/EAIM" target="_blank" style="font-weight:bold;  background-color:#2BA560; color:white;">
					  OpenReview – EAIM Workshop
				   </a>
				   <br/>
				   <span style="color:#F76C3B; font-weight:bold;">
					  Note: To sign up for OpenReview, you must use a university or institution email address. 
					  If you are not affiliated with any institution, account creation with a personal email may take up to 2 weeks so please plan accordingly. 
				   </span>
				  </p>
				  <p>
				   <b>Submission Policies</b><br/>
				   <b><span style="color:#F76C3B;">
					Submissions will follow a double-blind policy and must be prepared for anonymous review; authors should remove names, affiliations, and any links or references that could reveal their identity.
					</span></b> <br/><br/>
				   Authors must ensure that all submissions are original and have not been published elsewhere or concurrently submitted to another venue. 
				   Plagiarism in any form, including self-plagiarism, will result in desk rejection. <br/><br/>
				   If generative AI tools are used to produce substantial portions of the writing, analysis, or figures, their use must be clearly disclosed in the paper’s acknowledgments or methodology section. Authors remain fully responsible for the correctness, originality, and integrity of their submissions. <br/><br/>
				   At least one author of each accepted paper must register and present the work at the workshop. 
				</p>
               </div>
             </div>
         </div>
      </div>
      <!-- Speakers -->
      <div class="clients-container portfolio-container section-container" id="portfolio">
         <div class="container">
            <div class="row">
               <div class="col-sm-12 about section-description wow fadeIn">
                  <h3>Speakers</h3>
                  <div class="divider-1 wow fadeInUp"><span></span></div>
               </div>
            </div>
         </div>
         <!-- Example Speaker -->
         <div class="block-2-container section-container about-block-2-container">
            <div class="container">
               <div class="row">
                  <div class="col-sm-4 block-2-box block-2-left block-2-media wow fadeInLeft">
                     <div class="block-2-img-container">
                        <img src="assets/img/about/ethan.png" alt="Ethan Manilow">
                        <div class="img-container-line line-1"></div>
                        <div class="img-container-line line-2"></div>
                        <div class="img-container-line line-3"></div>
                     </div>
                  </div>
                  <div class="col-sm-8 block-2-box block-2-right wow fadeInUp">
                     <h3>Dr. Ethan Manilow / <span>Google DeepMind</span></h3>
                     <p><b>Bio</b> Ethan is currently a Senior Research Scientist at Google DeepMind on the Magenta Team. He finished his PhD in Computer Science working under Bryan Pardo in the Interactive Audio Lab at Northwestern University. During his PhD, he spent two years as a Student Researcher with Magenta and prior to that, he spent a year and half as Student Researcher at MERL on the Speech and Audio Team. His research centers on making machine learning systems that listen to and understand musical audio in an effort to make tools that can better assist artists.  
					 </p>
                  </div>
               </div>
            </div>
         </div>
		 <!-- Speaker: Dr. Elio Quinton -->
		<div class="block-2-container section-container section-container-gray about-block-2-container">
		   <div class="container">
			  <div class="row">
				 <div class="col-sm-8 block-2-box block-2-left wow fadeInLeft">
					<h3>Dr. Elio Quinton / <span>Universal Music Group</span></h3>
					<p>
					   <b>Bio</b> Elio is a scientist, engineer and leader with over a decade of experience in Artificial Intelligence, Machine Learning, and Audio Technology. Currently VP of Artificial Intelligence at Universal Music Group (UMG) and advisor to creative AI startups, he founded and leads the Music & Audio Machine Learning Lab (MAML), the first ever Machine Learning R\&D group in the recorded music industry. MAML's mission is to invent and build next-generation AI/ML tools to support and empower artists and industry professionals globally. Trained as both a scientist, engineer and musician, Elio holds a PhD in ML and Audio DSP from the Center for Digital Music, a Physics MSc, a Music Technology MA, and a diploma in Commercial Music performance from BIMM London.
					</p>
				 </div>
				 <div class="col-sm-4 block-2-box block-2-right wow fadeInLeft">
					<div class="block-2-img-container">
					   <img src="assets/img/about/elio2.png" alt="Elio Quinton">
					   <div class="img-container-line line-1"></div>
					   <div class="img-container-line line-2"></div>
					   <div class="img-container-line line-3"></div>
					</div>
				 </div>
			  </div>
		   </div>
		</div>
		<!-- Speaker: Prof. Dorien Herremans -->
		<div class="block-2-container section-container about-block-2-container">
		   <div class="container">
			  <div class="row">
				 <div class="col-sm-4 block-2-box block-2-left block-2-media wow fadeInLeft">
					<div class="block-2-img-container">
					   <img src="assets/img/about/dorien.png" alt="Prof. Dorien Herremans">
					   <div class="img-container-line line-1"></div>
					   <div class="img-container-line line-2"></div>
					   <div class="img-container-line line-3"></div>
					</div>
				 </div>
				 <div class="col-sm-8 block-2-box block-2-right wow fadeInUp">
					<h3>Prof. Dorien Herremans / <span>Singapore University of Technology and Design</span></h3>
					<p>
					   <b>Bio</b> Dorien is an Associate Professor at Singapore University of Technology and Design (SUTD), where she leads the Audio, Music, and AI (AMAAI) Lab. Before joining SUTD, she was a Marie Sklodowska-Curie Postdoctoral Fellow at the Centre for Digital Music at Queen Mary University of London. She was also nominated on the Singapore 100 Women in Tech list in 2021, and one of the top 30 SAIL Award (Super AI Leader) Finalists in 2024 at the World AI Conference.
					</p>
				 </div>
			  </div>
		   </div>
		</div>
		<!-- Speaker: Dr. Hanoi Hantrakul -->
		<div class="block-2-container section-container section-container-gray about-block-2-container">
		   <div class="container">
			  <div class="row">
				 <div class="col-sm-8 block-2-box block-2-left wow fadeInLeft">
					<h3>Dr. Hanoi Hantrakul / <span>ex-Google, ByteDance, TikTok</span></h3>
					<p>
					   <b>Bio</b> Lamtharn "Hanoi" Hantrakul is an AI Research Scientist and AI Sound Artist based in Bangkok, Thailand. His work explores how machine learning can empower music, arts and culture, particularly from Southeast Asia. With over 8 years of experience in the tech industry, he has developed state-of-the-art Generative AI models at Google, TikTok and ByteDance as a Senior AI Research Scientist. He is co-inventor of notable technologies including Google's open source DDSP library and the music LLM SEED-MUSIC deployed in Doubao (China's ChatGPT).
					   As a sound artist performing under "yaboihanoi," his electronic music incorporates Thai tunings and rhythms. He won the 2022 international AI Song Contest with "Enter Demons and Gods" and has performed at SONAR Music Festival alongside artists like Skrillex and Four Tet. His musical instrument "Fidular" received an A' Silver Award and Core77 Design Award in 2017 and is permanently exhibited at the Musical Instruments Museum in Phoenix, AZ. His work on machine learning and cultural empowerment has been covered by international media including Deutschlandfunk, Scientific American and Fast Company.
					</p>
				 </div>
				 <div class="col-sm-4 block-2-box block-2-right wow fadeInLeft">
					<div class="block-2-img-container">
					   <img src="assets/img/about/hanoi.jpeg" alt="Dr. Hanoi Hantrakul">
					   <div class="img-container-line line-1"></div>
					   <div class="img-container-line line-2"></div>
					   <div class="img-container-line line-3"></div>
					</div>
				 </div>
			  </div>
		   </div>
		</div>
		<!-- Speaker: Ziqian Ning -->
		<div class="block-2-container section-container about-block-2-container">
		   <div class="container">
			  <div class="row">
				 <div class="col-sm-8 block-2-box block-2-left wow fadeInLeft">
					<h3>Ziqian Ning / <span>ByteDance-Seed & ASLP@NPU</span></h3>
					<p>
					   <b>Bio</b> Ziqian Ning is an AI Research Scientist at ByteDance-Seed, where he develops large-scale music generation models. His research focuses on generative AI for music and speech, with expertise spanning music generation, voice conversion, and text-to-speech synthesis. Prior to ByteDance, he worked at Microsoft Azure Speech and Netease Fuxi AI Lab, developing real-time voice conversion systems and expressive speech technologies. He holds a Master's degree from Northwestern Polytechnical University under Prof. Lei Xie. His work has been published at leading conferences including AAAI, ICASSP, and INTERSPEECH. Notable contributions include DiffRhythm, the first diffusion-based model for end-to-end full-length song generation, and the DualVC series for low-latency streaming voice conversion. His team achieved first place in the Singing Voice Conversion Challenge 2023, demonstrating human-level naturalness. He continues to explore how AI can transform music creation and empower artists.
					</p>
				 </div>
				 <div class="col-sm-4 block-2-box block-2-right wow fadeInLeft">
					<div class="block-2-img-container">
					   <img src="assets/img/about/Ziqian_Ning.jpg" alt="Ziqian Ning">
					   <div class="img-container-line line-1"></div>
					   <div class="img-container-line line-2"></div>
					   <div class="img-container-line line-3"></div>
					</div>
				 </div>
			  </div>
		   </div>
		</div>
		<!-- End of Speakers Section -->


      <!-- Organizers -->
      <div class="about-container section-container" id="about">
         <div class="container">
            <div class="row">
               <div class="col-sm-12 about section-description wow fadeIn">
                  <h3>Organizers</h3>
                  <div class="divider-1 wow fadeInUp"><span></span></div>
               </div>
            </div>
         </div>
         <!-- Example Organizer -->
         <div class="block-2-container section-container about-block-2-container">
            <div class="container">
               <div class="row">
                  <div class="col-sm-4 block-2-box block-2-left block-2-media wow fadeInLeft">
                     <div class="block-2-img-container">
                        <img src="assets/img/about/keshav.jpg" alt="Keshav Bhandari">
                     </div>
                  </div>
                  <div class="col-sm-8 block-2-box block-2-right wow fadeInUp">
                     <h3>Keshav Bhandari / <span>Queen Mary University of London</span></h3>
                     <p>Keshav is a PhD candidate at the Centre for Digital Music (C4DM) at Queen Mary University of London supervised by Prof. Simon Colton. His research explores neuro-symbolic methods for music composition with a focus on musical structure and controllability. Prior to Queen Mary, he was part of the Interactive Audio Lab at Northwestern University, Evanston. Over the years, Keshav has published papers across conferences such as NeurIPS, AAAI, IJCNN and recently won the best paper award at EvoMUSART-25 (part of EvoStar). Keshav will be the main contact for the proposed AI music workshop at AAAI-26.</p>
                  </div>
               </div>
            </div>
         </div>
		 <!-- Organizer: Dr. Abhinaba Roy -->
		<div class="block-2-container section-container section-container-gray about-block-2-container">
		   <div class="container">
			  <div class="row">
				 <div class="col-sm-8 block-2-box block-2-left wow fadeInLeft">
					<h3>Dr. Abhinaba Roy / <span>Singapore University of Technology and Design</span></h3>
					<p>
					   Abhinaba is a senior research fellow at Singapore University of Technology and Design. He received his Ph.D. in Computer Vision in 2019 from the Istituto Italiano di Tecnologia, Genoa, Italy. He has held positions in both industry and academia, focusing on developing and deploying practical AI solutions. In recent years, his research has increasingly focused on the intersection of AI and music, particularly in areas such as text-to-music generation, symbolic music creation, and multimodal music understanding. His work has been published in leading conferences including ISMIR, IJCNN, AAAI, and others.
					</p>
				 </div>
				 <div class="col-sm-4 block-2-box block-2-right block-2-media wow fadeInUp">
					<div class="block-2-img-container">
					   <img src="assets/img/about/roy.jpeg" alt="Dr. Abhinaba Roy">
					   <div class="img-container-line line-1"></div>
					   <div class="img-container-line line-2"></div>
					   <div class="img-container-line line-3"></div>
					</div>
				 </div>
			  </div>
		   </div>
		</div>
		<!-- Organizer: Prof. Simon Colton -->
		<div class="block-2-container section-container about-block-2-container">
		   <div class="container">
			  <div class="row">
				 <div class="col-sm-4 block-2-box block-2-left block-2-media wow fadeInLeft">
					<div class="block-2-img-container">
					   <img src="assets/img/about/simon.jpeg" alt="Prof. Simon Colton">
					   <div class="img-container-line line-1"></div>
					   <div class="img-container-line line-2"></div>
					   <div class="img-container-line line-3"></div>
					</div>
				 </div>
				 <div class="col-sm-8 block-2-box block-2-right wow fadeInUp">
					<h3>Prof. Simon Colton / <span>Queen Mary University of London</span></h3>
					<p>
					   Simon is a Professor of Computational Creativity, AI and Games in EECS at Queen Mary University of London. He was previously an EPSRC leadership fellow at Imperial College and Goldsmiths College, and held an ERA Chair at Falmouth University. He is an AI researcher with around 200 publications whose work has won national and international awards, and has led numerous EPSRC and EU-funded projects. He focuses specifically on questions of Computational Creativity, where researchers study how to engineer systems to take on creative responsibilities in generative music, arts and science projects. Prof. Colton has has written about the philosophy of Computational Creativity and led numerous public engagement projects.
					</p>
				 </div>
			  </div>
		   </div>
		</div>
		<!-- Organizer: Prof. Dorien Herremans -->
		<div class="block-2-container section-container section-container-gray about-block-2-container">
		   <div class="container">
			  <div class="row">
				 <div class="col-sm-8 block-2-box block-2-left wow fadeInLeft">
					<h3>Prof. Dorien Herremans / <span>Singapore University of Technology and Design</span></h3>
					<p>
					   Dorien is an Associate Professor at Singapore University of Technology and Design (SUTD), where she leads the Audio, Music, and AI (AMAAI) Lab. Before joining SUTD, she was a Marie Sklodowska-Curie Postdoctoral Fellow at the Centre for Digital Music at Queen Mary University of London. She was also nominated on the Singapore 100 Women in Tech list in 2021, and one of the top 30 SAIL Award (Super AI Leader) Finalists in 2024 at the World AI Conference.
					</p>
				 </div>
				 <div class="col-sm-4 block-2-box block-2-right block-2-media wow fadeInUp">
					<div class="block-2-img-container">
					   <img src="assets/img/about/dorienh.jpg" alt="Prof. Dorien Herremans">
					   <div class="img-container-line line-1"></div>
					   <div class="img-container-line line-2"></div>
					   <div class="img-container-line line-3"></div>
					</div>
				 </div>
			  </div>
		   </div>
		</div>
         <!-- Add other organizers as before -->
      </div>
	  <!-- Programme Committee -->
		<div class="about-container section-container" id="committee">
		   <div class="container">
			  <div class="row">
				 <div class="col-sm-12 about section-description wow fadeIn">
					<h3>Programme Committee</h3>
					<div class="divider-1 wow fadeInUp"><span></span></div>
					<p style="text-align:left; font-size:1.5em; line-height:1.6;">
					   Prof. Dorien Herremans, Singapore University of Technology and Design <br/>
					   Keshav Bhandari, PhD candidate, Queen Mary University of London <br/>
					   Dr. Abhinaba Roy, Singapore University of Technology and Design <br/>
					   Prof. Simon Colton, Queen Mary University of London <br/>
					   Prof. Mathieu Barthet, Queen Mary University of London <br/>
					   Dr. Jaeyong Kang, Singapore University of Technology and Design <br/>
					   Dr. Jan Melechovsky, Singapore University of Technology and Design <br/>
					   Dr. Berker Banar, Ellison Institute of Technology Oxford <br/>
					   Benjamin Hayes, Sony CSL, Paris <br/>
					   Shuoyang Zheng, PhD candidate, Queen Mary University of London <br/>
					   MA Yinghao, PhD candidate, Queen Mary University of London <br/>
					   Zixun Guo, PhD candidate, Queen Mary University of London <br/>
					   Jordie Shier, PhD candidate, Queen Mary University of London <br/>			
					   <br/><br/>
					</p>
				 </div>
			  </div>
		   </div>
		</div>
      <!-- Contact -->
      <div class="block-2-container section-container contact-container" id="contact">
         <div class="container">
            <div class="row">
               <div class="col-sm-12 block-2 section-description wow fadeIn">
                  <h3>Contact Us</h3>
                  <div class="divider-1 wow fadeInUp"><span></span></div>
                  <p>You can contact the organizers at: k [dot] bhandari [at] qmul.ac.uk</p>
               </div>
            </div>
         </div>
      </div>
      <!-- Footer -->
      <footer>
         <div class="container">
            <div class="row">
               <div class="col-sm-12">
                  <div class="scroll-to-top">
                     <a href="#"><i class="fa fa-chevron-up"></i></a>
                  </div>
               </div>
            </div>
            <div class="row">
               <div class="col-sm-7 footer-copyright">
                  &copy; 2026 Workshop on Emerging AI Technologies for Music (EAIM2026).
               </div>
            </div>
         </div>
      </footer>
      <!-- Javascript -->
      <script src="assets/js/jquery-1.11.1.min.js"></script>
      <script src="assets/bootstrap/js/bootstrap.min.js"></script>
      <script src="assets/js/jquery.backstretch.min.js"></script>
      <script src="assets/js/wow.min.js"></script>
      <script src="assets/js/retina-1.1.0.min.js"></script>
      <script src="assets/js/jquery.magnific-popup.min.js"></script>
      <script src="assets/js/waypoints.min.js"></script>
      <script src="assets/js/jquery.countTo.js"></script>
      <script src="assets/js/masonry.pkgd.min.js"></script>
      <script src="assets/js/scripts.js"></script>
   </body>
</html>
